{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpim\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"local_zip = '/home/fabian/transportation.zip' #Zip File Location Path\\nzip_ref = zipfile.ZipFile(local_zip, 'r') # Read the local_zip\\nzip_ref.extractall('/home/fabian/') # All images will be located in /tmp/horse-or-man\\nzip_ref.close() # Close the File\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"local_zip = '/home/fabian/transportation.zip' #Zip File Location Path\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r') # Read the local_zip\n",
    "zip_ref.extractall('/home/fabian/') # All images will be located in /tmp/horse-or-man\n",
    "zip_ref.close() # Close the File\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plane = os.path.join('/home/fabian/dataset/transportation/plane')\n",
    "train_car = os.path.join('/home/fabian/dataset/transportation/car')\n",
    "\n",
    "validation_plane = os.path.join('/home/fabian/dataset/transval/plane')\n",
    "validation_car = os.path.join('/home/fabian/dataset/transval/car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alitalia1-700x467.png', '20200424-virgin-australia_4A8E536BCC6E4836A93C4076691909BD.jpg', 'b-17_sentimental_journey_204.jpg', 'Aeromexico-Boeing-787-Faces-Speed-Problems.jpg', 'Russian_Jets_Alaska_86236.jpg-a03e8.jpg', '19-July-Super-Hornet-625.png', 'images33.jpg', 'hqdefault.jpg', '23bc9965180b35fd007919c289e608d3.jpg', '_110649407_mediaitem110649406.jpg']\n",
      "['porsche-cayenne.jpg', 'model-s-range-desktop.png', '2020HYC010001_640_01.png', '945812.jpg', 'asset.MQ6.0.20190822163300.jpeg', '757610.jpg', '5e8b34a9883a9-bmw-eastern-region_20190930123821.png', '2019_Hyundai_Elantra_Limited_28AD_facelift29_front_NYIAS_2019.jpg', '2021-genesis-gv80-chicago-01.jpg', 'hyundai-palisade-menu.png']\n"
     ]
    }
   ],
   "source": [
    "train_plane_names = os.listdir(train_plane)\n",
    "print(train_plane_names[:10])\n",
    "\n",
    "train_car_names = os.listdir(train_car)\n",
    "print(train_car_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training plane images: 827\n",
      "total training car images: 943\n"
     ]
    }
   ],
   "source": [
    "print('total training plane images:', len(os.listdir(train_plane)))\n",
    "print('total training car images:', len(os.listdir(train_car)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 100\n",
    "next_plane = [os.path.join(train_plane, fname) for fname in train_plane[pic_index-8:pic_index]]\n",
    "next_car = [os.path.join(train_car, fname) for fname in train_car[pic_index-8:pic_index]]\n",
    "\n",
    "next_plane\n",
    "next_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_path in enumerate(next_plane+next_car):\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('Off') \n",
    "\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3528 images belonging to 2 classes.\n",
      "Found 165 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = image.ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/fabian/dataset/transportation/',  \n",
    "        target_size=(300, 300),  \n",
    "        batch_size=50,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '/home/fabian/dataset/transval/',\n",
    "        target_size=(300, 300), \n",
    "        batch_size=10,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 20 steps, validate for 8 steps\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 36s 2s/step - loss: 0.2970 - accuracy: 0.8820 - val_loss: 0.3213 - val_accuracy: 0.8625\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.2438 - accuracy: 0.9040 - val_loss: 0.2940 - val_accuracy: 0.8625\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 36s 2s/step - loss: 0.1945 - accuracy: 0.9190 - val_loss: 0.2848 - val_accuracy: 0.8750\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.2340 - accuracy: 0.9080 - val_loss: 0.2501 - val_accuracy: 0.8625\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.2253 - accuracy: 0.9140 - val_loss: 0.2167 - val_accuracy: 0.9000\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.2118 - accuracy: 0.9210 - val_loss: 0.2455 - val_accuracy: 0.9000\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.1802 - accuracy: 0.9330 - val_loss: 0.1938 - val_accuracy: 0.9000\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.1776 - accuracy: 0.9305 - val_loss: 0.1631 - val_accuracy: 0.9250\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.1584 - accuracy: 0.9376 - val_loss: 0.1379 - val_accuracy: 0.9500\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.1201 - accuracy: 0.9590 - val_loss: 0.1739 - val_accuracy: 0.9250\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.1115 - accuracy: 0.9640 - val_loss: 0.1942 - val_accuracy: 0.9250\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.1355 - accuracy: 0.9480 - val_loss: 0.1387 - val_accuracy: 0.9500\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.1583 - accuracy: 0.9346 - val_loss: 0.1552 - val_accuracy: 0.9500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.1271 - accuracy: 0.9530 - val_loss: 0.1214 - val_accuracy: 0.9500\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.1065 - accuracy: 0.9570 - val_loss: 0.1674 - val_accuracy: 0.9250\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.0866 - accuracy: 0.9632 - val_loss: 0.1125 - val_accuracy: 0.9625\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.0647 - accuracy: 0.9740 - val_loss: 0.1058 - val_accuracy: 0.9750\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.0956 - accuracy: 0.9630 - val_loss: 0.0877 - val_accuracy: 0.9625\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.0794 - accuracy: 0.9673 - val_loss: 0.1453 - val_accuracy: 0.9625\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.0630 - accuracy: 0.9760 - val_loss: 0.1073 - val_accuracy: 0.9750\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.1709 - val_accuracy: 0.9625\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.0186 - accuracy: 0.9959 - val_loss: 0.1512 - val_accuracy: 0.9750\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.0554 - accuracy: 0.9806 - val_loss: 0.1829 - val_accuracy: 0.9625\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.2710 - val_accuracy: 0.9375\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.0362 - accuracy: 0.9860 - val_loss: 0.0973 - val_accuracy: 0.9750\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.0256 - accuracy: 0.9910 - val_loss: 0.1039 - val_accuracy: 0.9750\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.0202 - accuracy: 0.9910 - val_loss: 0.2048 - val_accuracy: 0.9375\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.0196 - accuracy: 0.9928 - val_loss: 0.1761 - val_accuracy: 0.9875\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.0154 - accuracy: 0.9970 - val_loss: 0.1602 - val_accuracy: 0.9750\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.1283 - val_accuracy: 0.9750\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.0337 - accuracy: 0.9910 - val_loss: 0.1410 - val_accuracy: 0.9750\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.0191 - accuracy: 0.9910 - val_loss: 0.1561 - val_accuracy: 0.9625\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.0158 - accuracy: 0.9970 - val_loss: 0.1527 - val_accuracy: 0.9625\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.0257 - accuracy: 0.9930 - val_loss: 0.1511 - val_accuracy: 0.9875\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.0084 - accuracy: 0.9980 - val_loss: 0.1334 - val_accuracy: 0.9875\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1445 - val_accuracy: 0.9875\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1696 - val_accuracy: 0.9875\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 30s 2s/step - loss: 6.3338e-04 - accuracy: 1.0000 - val_loss: 0.1700 - val_accuracy: 0.9750\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 35s 2s/step - loss: 5.5530e-04 - accuracy: 1.0000 - val_loss: 0.1871 - val_accuracy: 0.9875\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 35s 2s/step - loss: 3.5279e-04 - accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.9875\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 31s 2s/step - loss: 5.4133e-04 - accuracy: 1.0000 - val_loss: 0.1983 - val_accuracy: 0.9750\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 2.4910e-04 - accuracy: 1.0000 - val_loss: 0.2093 - val_accuracy: 0.9875\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 35s 2s/step - loss: 1.7588e-04 - accuracy: 1.0000 - val_loss: 0.2135 - val_accuracy: 0.9875\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 1.5353e-04 - accuracy: 1.0000 - val_loss: 0.2142 - val_accuracy: 0.9750\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 1.5632e-04 - accuracy: 1.0000 - val_loss: 0.2194 - val_accuracy: 0.9875\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 1.7165e-04 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9875\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 1.0405e-04 - accuracy: 1.0000 - val_loss: 0.2257 - val_accuracy: 0.9750\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 32s 2s/step - loss: 1.4962e-04 - accuracy: 1.0000 - val_loss: 0.2282 - val_accuracy: 0.9750\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 33s 2s/step - loss: 8.6701e-05 - accuracy: 1.0000 - val_loss: 0.2340 - val_accuracy: 0.9875\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 34s 2s/step - loss: 1.2413e-04 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=20, epochs=50, verbose=1, validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "/home/fabian/car.jpg is a car\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = '/home/fabian/car.jpg'\n",
    "img = image.load_img(path, target_size=(300,300,3))\n",
    "x = image.img_to_array(img)\n",
    "    \n",
    "x=np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "\n",
    "classes = model.predict(images, batch_size=10)\n",
    "\n",
    "print(classes[0])\n",
    "\n",
    "if classes[0]>0:\n",
    "    print(path + \" is a plane\")\n",
    "else:\n",
    "    print(path + \" is a car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing Overall Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Overall Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing Overall Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Overall Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data was overfitted around the 20th epoch; therefore, the training loss continues to decrease unlike the test loss, which increased.\n",
    "- Spikes on the validation graph denotes that the validation dataset is too small, causing some rough changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
