{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpim\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"local_zip = '/home/fabian/transportation.zip' #Zip File Location Path\\nzip_ref = zipfile.ZipFile(local_zip, 'r') # Read the local_zip\\nzip_ref.extractall('/home/fabian/') # All images will be located in /tmp/horse-or-man\\nzip_ref.close() # Close the File\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"local_zip = '/home/fabian/transportation.zip' #Zip File Location Path\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r') # Read the local_zip\n",
    "zip_ref.extractall('/home/fabian/') # All images will be located in /tmp/horse-or-man\n",
    "zip_ref.close() # Close the File\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plane = os.path.join('/home/fabian/transportation/plane')\n",
    "train_car = os.path.join('/home/fabian/transportation/car')\n",
    "\n",
    "validation_plane = os.path.join('/home/fabian/transval/plane')\n",
    "validation_car = os.path.join('/home/fabian/transval/car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alitalia1-700x467.png', '20200424-virgin-australia_4A8E536BCC6E4836A93C4076691909BD.jpg', 'b-17_sentimental_journey_204.jpg', 'Aeromexico-Boeing-787-Faces-Speed-Problems.jpg', 'Russian_Jets_Alaska_86236.jpg-a03e8.jpg', '19-July-Super-Hornet-625.png', 'images33.jpg', 'hqdefault.jpg', '23bc9965180b35fd007919c289e608d3.jpg', '_110649407_mediaitem110649406.jpg']\n",
      "['porsche-cayenne.jpg', 'model-s-range-desktop.png', '2020HYC010001_640_01.png', '945812.jpg', 'asset.MQ6.0.20190822163300.jpeg', '757610.jpg', '5e8b34a9883a9-bmw-eastern-region_20190930123821.png', '2019_Hyundai_Elantra_Limited_28AD_facelift29_front_NYIAS_2019.jpg', '2021-genesis-gv80-chicago-01.jpg', 'hyundai-palisade-menu.png']\n"
     ]
    }
   ],
   "source": [
    "train_plane_names = os.listdir(train_plane)\n",
    "print(train_plane_names[:10])\n",
    "\n",
    "train_car_names = os.listdir(train_car)\n",
    "print(train_car_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training plane images: 827\n",
      "total training car images: 943\n"
     ]
    }
   ],
   "source": [
    "print('total training plane images:', len(os.listdir(train_plane)))\n",
    "print('total training car images:', len(os.listdir(train_car)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 100\n",
    "next_plane = [os.path.join(train_plane, fname) for fname in train_plane[pic_index-8:pic_index]]\n",
    "next_car = [os.path.join(train_car, fname) for fname in train_car[pic_index-8:pic_index]]\n",
    "\n",
    "next_plane\n",
    "next_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_path in enumerate(next_plane+next_car):\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('Off') \n",
    "\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = RMSprop(lr=0.001),\n",
    "              metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3528 images belonging to 2 classes.\n",
      "Found 165 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/fabian/transportation/',  \n",
    "        target_size=(300, 300),  \n",
    "        batch_size=100,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '/home/fabian/transval/',\n",
    "        target_size=(300, 300), \n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 3 steps, validate for 8 steps\n",
      "Epoch 1/100\n",
      "2/3 [===================>..........] - ETA: 3s - loss: 3.2061 - accuracy: 0.4950 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 18s 6s/step - loss: 2.3772 - accuracy: 0.4633 - val_loss: 0.7103 - val_accuracy: 0.4688\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 17s 6s/step - loss: 0.6920 - accuracy: 0.5600 - val_loss: 0.8215 - val_accuracy: 0.4688\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.7916 - accuracy: 0.6033 - val_loss: 0.6752 - val_accuracy: 0.5375\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.6981 - accuracy: 0.4967 - val_loss: 0.6557 - val_accuracy: 0.7312\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 17s 6s/step - loss: 0.6412 - accuracy: 0.7433 - val_loss: 0.7638 - val_accuracy: 0.4688\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.6057 - accuracy: 0.6733 - val_loss: 0.7348 - val_accuracy: 0.7312\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.5153 - accuracy: 0.7500 - val_loss: 0.5473 - val_accuracy: 0.7750\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 17s 6s/step - loss: 0.4610 - accuracy: 0.7851 - val_loss: 0.4723 - val_accuracy: 0.7688\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.6034 - accuracy: 0.6867 - val_loss: 0.4577 - val_accuracy: 0.8687\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.4366 - accuracy: 0.8500 - val_loss: 0.3703 - val_accuracy: 0.8625\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 20s 7s/step - loss: 0.3909 - accuracy: 0.8633 - val_loss: 0.4799 - val_accuracy: 0.8375\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 17s 6s/step - loss: 0.5087 - accuracy: 0.8033 - val_loss: 0.3977 - val_accuracy: 0.8938\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.3470 - accuracy: 0.8700 - val_loss: 0.3382 - val_accuracy: 0.8750\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 17s 6s/step - loss: 0.3773 - accuracy: 0.8633 - val_loss: 1.3492 - val_accuracy: 0.5125\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.5943 - accuracy: 0.8067 - val_loss: 0.3669 - val_accuracy: 0.8875\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.2982 - accuracy: 0.8833 - val_loss: 0.3641 - val_accuracy: 0.8687\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.6125 - accuracy: 0.7800 - val_loss: 1.2899 - val_accuracy: 0.5312\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 20s 7s/step - loss: 0.6350 - accuracy: 0.7967 - val_loss: 0.3832 - val_accuracy: 0.8813\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2868 - accuracy: 0.9100 - val_loss: 0.3385 - val_accuracy: 0.8750\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.2626 - accuracy: 0.8967 - val_loss: 0.3850 - val_accuracy: 0.8687\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.3743 - accuracy: 0.8500 - val_loss: 0.7789 - val_accuracy: 0.6812\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 17s 6s/step - loss: 0.4045 - accuracy: 0.8533 - val_loss: 0.3760 - val_accuracy: 0.8625\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2470 - accuracy: 0.9100 - val_loss: 0.4752 - val_accuracy: 0.8250\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.3273 - accuracy: 0.8567 - val_loss: 0.3003 - val_accuracy: 0.8687\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.3635 - accuracy: 0.8500 - val_loss: 0.3616 - val_accuracy: 0.8687\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2557 - accuracy: 0.9200 - val_loss: 0.3513 - val_accuracy: 0.8687\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2187 - accuracy: 0.9033 - val_loss: 0.3105 - val_accuracy: 0.8813\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.7526 - accuracy: 0.7167 - val_loss: 0.4871 - val_accuracy: 0.7875\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 17s 6s/step - loss: 0.3665 - accuracy: 0.8933 - val_loss: 0.3793 - val_accuracy: 0.8625\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2962 - accuracy: 0.8900 - val_loss: 0.3763 - val_accuracy: 0.8625\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2457 - accuracy: 0.9267 - val_loss: 0.2874 - val_accuracy: 0.8750\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.3435 - accuracy: 0.8700 - val_loss: 0.3247 - val_accuracy: 0.8875\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2976 - accuracy: 0.8867 - val_loss: 0.2922 - val_accuracy: 0.8750\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.2818 - accuracy: 0.8700 - val_loss: 0.4085 - val_accuracy: 0.8313\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.3025 - accuracy: 0.8767 - val_loss: 0.2728 - val_accuracy: 0.8938\n",
      "Epoch 36/100\n",
      "2/3 [===================>..........] - ETA: 4s - loss: 0.3415 - accuracy: 0.8650 "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=3, epochs=100, verbose=1, validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
